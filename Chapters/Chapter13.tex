\chapter{Screenreader Accessible Graphics and Interactive Data Environments}\label{ch13:accessible-graphics}
\glsreset{ocr}\glsreset{icr}\glsreset{tts}\glsreset{llm}\glsreset{uia}\glsreset{msaa}\glsreset{pdfua}\glsreset{api}\glsreset{cpu}
\raggedright
\index{data visualization!accessible}\gidx{tactilegraphics}{tactile graphics}\index{haptic feedback}\index{sonification}\gidx{screenreader}{screen reader}\index{AI}

\section{~~Overview}\label{ch13:sec:overview}
Accessible data visualization integrates multiple sensory channels—tactile, auditory, haptic, and natural language—to allow learners who are blind or have low vision to perceive structure, trends, and insight normally conveyed visually. This chapter reframes prior narrative content into a pedagogical structure: establishing learning objectives, defining key terms, mapping standards, providing implementation strategies, identifying best practices and pitfalls, and presenting emerging multi-modal and AI-driven paradigms. The goal is to equip Teachers of Students with Visual Impairments (TVIs) and \gidx{accessibility}{accessibility} technologists with a repeatable workflow to select, design, and evaluate screen reader accessible graphics experiences.

\section{~~Learning Objectives}\label{ch13:sec:learning-objectives}
After completing this chapter, the reader will be able to:
\begin{enumerate}
	\item Explain and differentiate core non-visual modalities (tactile, sonification, haptic, natural language generation) for data exploration.
	\item Map data visualization accessibility decisions to relevant standards (WCAG non-text content, ARIA roles, \gidx{tactilegraphics}{tactile graphics} guidelines).
	\item Design an implementation workflow that combines at least two modalities with structured keyboard \gidx{navigation}{navigation} for an educational dataset.
	\item Diagnose and resolve common accessibility breakdowns (ambiguous alt text, overcrowded tactile layout, confusing sonification scale) using a structured troubleshooting matrix.
\end{enumerate}

\section{~~Key Terms}\label{ch13:sec:key-terms}
\begin{description}
	\item[Tactile Graphic] A raised-line or dynamic pin-based representation translating visual spatial relationships into a touch-readable format.
	\item[Sonification] Systematic mapping of quantitative data parameters to auditory variables (pitch, tempo, timbre, spatial position).
	\item[Haptic Feedback] Non-auditory vibro-tactile or force cues supplementing or disambiguating tactile/static information.
	\item[Refreshable Tactile Display (RTD)] A device with dynamic pin arrays enabling real-time reconfiguration of \gidx{tactilegraphics}{tactile graphics} and braille.
	\item[Natural Language Generation (NLG)] Automated production of textual summaries and semantic descriptions from underlying structured data.
	\item[Multi-Modal Fusion] Coordinated integration of two or more sensory channels (e.g., touch + audio) with synchronized interaction states.
	\item[Semantic Chart Description] Layered textual or spoken narrative (construction → evidence → repetition → generalization) conveying both structure and insight\supercite{VisText}.
\end{description}

\section{~~Historical and Policy Context}\label{ch13:sec:historical-policy}
Early accessibility interventions emphasized static embossed diagrams and separate text descriptions. Progression toward multi-modal interaction followed recognition that raw data tables alone did not support higher-order analytical reasoning\supercite{Lundgard2021, Zewe2021Making}. WCAG non-text content requirements and ARIA role evolution (e.g., graphics roles) coupled with research efforts (Accessible Graphs initiative\supercite{AccessibleGraphs, AccessVizWorkshop}) accelerated tooling for dynamic exploration. Emerging RTDs, richer haptics, and AI-driven summarization further shift expectations from basic access to equitable analytical engagement.

\section{~~Core Concepts}\label{ch13:sec:core-concepts}
The existing sections in this chapter (Executive Summary, modality-specific sections, interaction paradigms, infographic guidance, and cross-cutting themes) collectively articulate core concepts:
\begin{itemize}
	\item Multi-modality enhances comprehension by compensating for modality-specific limitations.
	\item Structured \gidx{navigation}{navigation} (keyboard focus order + semantic grouping) underpins efficient non-visual exploration\supercite{AutoVizuA11y}.
	\item Layered semantic descriptions (e.g., VisText model) promote scalable authoring\supercite{VisText, Lundgard2022Accessible}.
	\item User-centered iteration integrating blind user feedback remains essential for emergent modalities (e.g., haptics, AI-generated captions).
\end{itemize}

\section{~~Executive Summary}\label{ch13:sec:executive-summary}
This chapter provides a comprehensive review of the technologies and research advancing the accessibility of data visualizations\index{data visualization!accessible} for individuals with visual impairments. It examines a range of modalities, including \gidx{tactilegraphics}{tactile graphics}, \gidx{hapticfeedback}{haptic feedback}, sonification\index{sonification}, and natural language generation\index{natural language generation (NLG)}. The report highlights key academic and open-source projects, such as MIT's work on Tactile Vega-Lite and VisText, and discusses the critical role of AI\index{AI!in data accessibility} and multi-modal interfaces in creating rich, interactive data exploration experiences. The analysis concludes that a shift from static, single-mode solutions to dynamic, multi-modal, and AI-driven environments is essential for achieving true data accessibility.

\section{~~Implementation Strategies}\label{ch13:sec:implementation-strategies}
\subsection*{1. Needs Assessment}
Identify learner goals (trend identification, correlation comparison, categorical distribution). Select modalities that directly support those analytic tasks (e.g., sonification for continuous trend slope perception).
\subsection*{2. Data Preparation}
Normalize datasets (consistent units, descriptive variable names) and pre-compute statistically salient points (extrema, inflection points) for targeted NLG or haptic emphasis.
\subsection*{3. Multi-Modal Design}
Pair a primary exploration mode (tactile or sonification) with a summarization assist (NLG). Ensure synchronized focus: touching a tactile bar triggers proportional pitch or spoken label.
\subsection*{4. \gidx{navigation}{Navigation} Schema}
Define keyboard and \gidx{brailledisplay}{braille display} commands for: next/previous series, categorical jump, summary toggle, statistics on demand (mean, max).
\subsection*{5. Authoring Workflow}
Embed semantic layers early (ARIA roles, data attributes) → generate alt + long description drafts → refine via user testing (think-aloud sessions).
\subsection*{6. QA and Validation}
Checklist: perceivable tactile differentiation (minimum spacing), sonification mapping clarity (non-overlapping pitch range), description completeness (construction → generalization), \gidx{latency}{latency} < 250 ms for interactive auditory feedback.
\subsection*{7. Deployment and Iteration}
Collect usage metrics (time-to-insight, error rate in identifying trend direction) to inform iterative refinements.

\section{~~Standards and Compliance}\label{ch13:sec:standards-compliance}
\begin{itemize}
	\item \textbf{WCAG 2.x Non-text Content (1.1.1):} Provide equivalent textual or multimodal alternatives\supercite{WCAG21LevelAA}.
	\item \textbf{WCAG 2.x Info and Relationships (1.3.1):} Preserve structural semantics via headings, ARIA roles.
	\item \textbf{Keyboard Accessibility (2.1.x):} Full chart interaction without pointer devices.
	\item \textbf{Contrast and Visual Alternatives (1.4.x):} For low vision users, ensure adjustable color/high contrast plus non-color redundancy\supercite{ContrastChecker}.
	\item \textbf{ARIA Graphics Roles (emerging practice):} Assign roles/labels for layered interactive regions.
	\item \textbf{\gidx{tactilegraphics}{Tactile Graphics} Guidelines (BANA / APH):} Standardize line styles, textures, labeling for embossed/RTD output\supercite{CreatingTactileGraphics}.
\end{itemize}

\section{~~Best Practices}\label{ch13:sec:best-practices}
\begin{itemize}
	\item Provide layered descriptions: brief alt text + expandable structured summary.
	\item Limit simultaneous channels: introduce modalities progressively to reduce cognitive overload.
	\item Distinguish tactile elements through texture + spacing (avoid overcrowding).
	\item Use consistent auditory mappings (pitch increases with Y value) across visualizations for learnability.
	\item Log user interaction paths to detect \gidx{navigation}{navigation} friction (repeated backtracking).
	\item Pilot with diverse blind users (braille-first, auditory-first) to uncover modality bias.
\end{itemize}

\section{~~Troubleshooting and Common Pitfalls}\label{ch13:sec:troubleshooting}
\footnotesize
\begin{longtblr}[
		caption = {Common Issues in Accessible Data Visualization Implementations},
		label = {ch13:tab:troubleshooting},
		note = {Structured remediation guidance for multi-modal chart accessibility.}
	]{
		colspec = {X[l] X[l] X[l] X[l] X[l] X[l]},
		rowhead = 1,
		row{1} = {font=\bfseries},
		hlines
	}
	Issue                                                & RootCause                                          & ImpactOnLearner                                & ResolutionSteps                                            & PreventivePractice                                       & ReferenceKey                \\
	Ambiguous or generic alt text                        & Auto-generation without semantic model layering    & Loss of context; misinterpretation             & Re-author using construction→evidence→generalization model & Apply VisText template during authoring                  & VisText                     \\
	Overcrowded tactile graphic                          & Excessive data density; insufficient spacing       & Difficulty parsing; increased exploration time & Aggregate categories; simplify textures; re-emboss test    & Set max tactile elements per area (e.g., <12 bars panel) & CreatingTactileGraphics     \\
	Confusing sonification pitch scale                   & Non-linear mapping or overlapping frequency ranges & Misread trends                                 & Standardize linear pitch mapping; add audio legend mode    & Document mapping schema; user training overlay           & Trayford2023STRAUSS         \\
	Latency in interactive audio feedback >250 ms        & Inefficient event loop or remote processing        & Broken mental model; reduced engagement        & Profile callbacks; cache data locally; debounce            & Performance budget in design spec                        & RichScreenReaderExperiences \\
	Haptic cues indistinguishable                        & Similar vibration patterns for distinct events     & Cognitive load; missed anomalies               & Redesign with distinct frequency/intensity envelopes       & Maintain pattern inventory; user A/B test                & HapticDataViz               \\
	Missing keyboard \gidx{navigation}{navigation} for granular exploration & Focus model lacks data point addressing            & Inaccessible fine-grained analysis             & Implement roving tabindex or ARIA grid pattern             & Early keyboard prototype prior to graphic styling        & AutoVizuA11y                \\
\end{longtblr}
\normalsize

\section{~~Emerging Trends and Future Directions}\label{ch13:sec:future-directions}
\subsection{Adaptive AI Summarization}
\gls{llm} and specialized NLG pipelines are moving toward adaptive descriptions tuned to user expertise (novice vs. advanced) and task intent\supercite{VisText, Lundgard2022Accessible}.
\subsection{Dynamic Multi-Line Tactile Displays}
Higher pin density RTDs promise real-time panning, multi-layer overlays (grid + data + annotations), and contextual zoom.
\subsection{Personalized Sonification}
User-calibrated frequency and spatialization profiles reduce learning curve and improve discrimination of similar data series\supercite{Trayford2023STRAUSS}.
\subsection{Semantic Interoperability}
Standardized JSON schemas (e.g., Vega-Lite tactile extensions) streamline multi-modal export and automated description generation\supercite{TactileVegaLite}.

\section{~~Ethical, Equity, and Privacy Considerations}\label{ch13:sec:ethics-equity}
Equitable data literacy requires parity in time-to-insight. Designs that increase exploration latency or omit higher-level trend abstraction create systemic inequities. Ethical deployment of AI-generated descriptions mandates transparency (flagging autogenerated content) and mitigation of bias in summarization (e.g., avoiding subjective language). For cloud-based analytics, protect sensitive educational data when transmitting to AI services; apply least-privilege data minimization.

\section{~~Assessment and Reflection}\label{ch13:sec:assessment-reflection}
\textbf{Reflection Questions}
\begin{enumerate}
	\item Which modality pairing (e.g., tactile + NLG vs. sonification + haptic) best supports comparative trend analysis in a multi-series dataset and why?
	\item How would you measure and reduce exploration latency for a complex line chart on an RTD device?
	\item What safeguards ensure AI-generated chart summaries remain faithful to underlying data without hallucination?
\end{enumerate}
\textbf{Applied Exercise} Select a small dataset (e.g., monthly temperature). Design a three-layer accessibility plan: (1) tactile layout spec, (2) sonification mapping table, (3) VisText-style description draft. Produce a brief QA checklist referencing the troubleshooting table.

\section{~~Summary}\label{ch13:sec:summary}
Accessible, screen reader friendly data visualization requires coordinated multi-modal design, robust semantic structuring, and iterative user-centered validation. By combining standardized description frameworks, consistent auditory/haptic mappings, tactile clarity, and performance budgets, educators can deliver equitable analytical experiences. Emerging AI and dynamic tactile \gidx{hardware}{hardware} promise richer personalization; governance (transparency, data privacy, bias mitigation) must accompany adoption to preserve trust.

% --- Original Detailed Sections (Preserved) ---
\section{~~Introduction: The Imperative of Accessible Data Visualization}\label{ch13:sec:introduction}
In an increasingly data-driven world, the ability to understand and interpret graphical representations of data is a fundamental form of literacy. However, for the millions of individuals with visual impairments, standard charts, graphs, and infographics present significant barriers. Making data visualization\index{data visualization!accessible} accessible is not merely a matter of compliance but a crucial step toward educational and professional equity. Projects like the Accessible Graphs initiative demonstrate ongoing efforts to develop comprehensive solutions for making data visualizations accessible to all users \supercite{AccessibleGraphs, AccessVizWorkshop}.

\subsection{Challenges for Visually Impaired Users}\label{ch13:ssec:challenges}
Visually impaired users, particularly those who rely on screen readers\index{screen reader}, face numerous challenges with graphical content. Screen readers\index{screen reader!limitations with graphics} can interpret text but cannot convey the spatial relationships, trends, and patterns inherent in a visual chart. Traditional solutions, such as providing raw data tables, are often insufficient as they fail to communicate the high-level insights that visualizations are designed to reveal \supercite{Lundgard2021, Zewe2021Making}.

\subsection{Evolution of Accessibility Solutions}\label{ch13:ssec:evolution}
The approach to data accessibility\index{data accessibility} has evolved from simple text descriptions to a rich ecosystem of multi-modal tools. Early efforts focused on static \gidx{tactilegraphics}{tactile graphics}, while modern research explores dynamic, interactive systems that combine touch, sound, and language to create a more comprehensive understanding of the data \supercite{Zewe2022Making, Lundgard2022Accessible}.

\section{~~\gidx{tactilegraphics}{Tactile Graphics} and Interactive Displays}\label{ch13:sec:tactile-graphics}
\gls{tactilegraphics}tactile graphics translate visual information into a format that can be read by touch. This is a foundational method for conveying the structure of charts and graphs \supercite{CreatingTactileGraphics}.

\subsection{MIT's Tactile Vega-Lite: Streamlining Tactile Chart Design}\label{ch13:ssec:tactile-vega-lite}
Researchers at MIT have developed tools to simplify the creation of high-quality \gidx{tactilegraphics}{tactile graphics}. The \textbf{Tactile Vega-Lite}\index{projects!MIT!Tactile Vega-Lite} project provides a system for automatically generating tactile chart designs from a high-level JSON\index{JSON} specification. This approach standardizes the design process, ensuring that tactile charts are clear, consistent, and optimized for touch. The system can output designs ready for 3D printing\index{3D printing!for tactile graphics} or embossing, lowering the barrier for educators and content creators to produce \gidx{accessiblematerials}{accessible materials} \supercite{TactileVegaLite}.

\subsection{Other Innovations in Tactile Data Representation}\label{ch13:ssec:tactile-innovations}
The field of tactile data representation\index{data representation!tactile} is continually evolving, with a focus on creating more dynamic and interactive experiences.

\subsubsection{Refreshable Tactile Displays (RTDs)}\label{ch13:sssec:rtds}
Refreshable tactile displays (RTDs)\index{hardware!refreshable tactile display}, such as those developed by Dot Inc. and other companies, are devices that use an array of movable pins to create dynamic tactile images and braille\index{braille!displays}. These displays have the potential to render data visualizations in real-time, allowing users to explore graphs interactively without the need for static, embossed copies. The integration of RTDs with data visualization \gidx{software}{software} is a key area of ongoing research \supercite{DotInc, OrbitGraphiti}.

\section{~~Haptic Feedback for Data Exploration}\label{ch13:sec:haptic-feedback}
\gls{hapticfeedback}\index{haptic feedback} uses vibrations, forces, or motions to provide non-visual information. This technology can complement \gidx{tactilegraphics}{tactile graphics} by adding another layer of data to the user's experience \supercite{HapticDataViz}.

\subsection{MIT's Haptic Footprint and Fifth Sense Project: Non-Visual Information Delivery}\label{ch13:ssec:haptic-footprint}
The \textbf{Haptic Footprint}\index{projects!MIT!Haptic Footprint} and \textbf{Fifth Sense}\index{projects!MIT!Fifth Sense} projects from MIT explore the use of haptic feedback\index{haptic feedback!non-visual information} to convey information. These systems can, for example, use vibrations to guide a user's hand over a tactile graphic, highlighting specific data points or trends. This approach can make complex charts easier to navigate and understand by providing dynamic cues \supercite{HapticFootprint, MITFifthSense}.

\subsection{TactStyle: Replicating Tactile Properties in 3D Models}\label{ch13:ssec:tactstyle}
The \textbf{TactStyle}\index{projects!TactStyle} project focuses on embedding haptic properties directly into 3D printed models\index{3D printing!haptic properties}. By using different printing techniques and materials, designers can create objects with varied textures, stiffness, and other tactile qualities. This allows for the creation of 3D charts and graphs where data values are represented by physical properties, providing a rich, multi-sensory method for data exploration\index{data exploration!haptic} \supercite{TactStyle}.

\section{~~Sonification: Auditory Data Representation}\label{ch13:sec:sonification}
\gls{sonification}\index{sonification} is the use of non-speech audio to convey information. In the context of data visualization, this means mapping data values to auditory properties like pitch, volume, or tempo.

\subsection{Umwelt: Multimodal Data Representations for Interactive Exploration}\label{ch13:ssec:umwelt}
The \textbf{Umwelt}\index{projects!Umwelt} project is a system that combines sonification with touch interaction to create a multi-modal\index{interaction paradigms!multimodal} data exploration experience. Users can trace a graph on a touchscreen or tactile overlay, and the system provides real-time auditory feedback corresponding to the data values. This allows for a fluid and intuitive way to understand trends, identify outliers, and compare different data series \supercite{Umwelt, Zong2024Umwelt}.

\subsection{Other Sonification Tools and Frameworks}\label{ch13:ssec:sonification-tools}
Many accessible graphing calculators, such as those from Desmos and ViewPlus, incorporate sonification\index{software!sonification} as a primary means of exploring graphs. These tools allow users to "hear" the shape of a function, making complex mathematical concepts more accessible \supercite{Trayford2023STRAUSS, SonificationSandbox, TwoTone}.

\section{~~Natural Language Generation (NLG) for Data Interpretation}\label{ch13:sec:nlg}
\gls{nlg}\index{natural language generation (NLG)} is a field of \gls{AI} that focuses on generating human-like text from structured data. For data visualization, this means automatically creating summaries and descriptions of charts.

\subsection{MIT's VisText and the Four-Level Model of Semantic Content}\label{ch13:ssec:vistext}
The \textbf{VisText}\index{projects!MIT!VisText} project from MIT proposes a four-level model for generating textual descriptions of data visualizations\index{data visualization!text descriptions}. This model breaks down the description into essential components:
\begin{enumerate}
	\item \textbf{Construction:} How the chart is built (e.g., "This is a bar chart with the x-axis representing months...").
	\item \textbf{Evidence:} Key data points and trends (e.g., "Sales peaked in June at \$50,000.").
	\item \textbf{Repetition:} Grouping and summarizing related information.
	\item \textbf{Generalization:} High-level insights and conclusions (e.g., "Overall, sales showed a strong upward trend in the second quarter.").
\end{enumerate}
This structured approach\index{natural language generation (NLG)!semantic content model} ensures that the generated text is comprehensive and easy to understand \supercite{VisText, Lundgard2022Accessible}.

\subsection{Automated Description Tools}\label{ch13:ssec:automated-description}
Several tools and research projects are leveraging \gls{AI}\index{AI!text generation} to automatically generate alt text and summaries for charts. These tools analyze the underlying data and visual structure to produce descriptions that can be read by screen readers, providing a scalable solution for making large numbers of charts accessible \supercite{Smits2024AltGosling, SeeChart}.

\subsection{Frameworks for Computer-Assisted Descriptive Narratives (``From Graphs to Words'')}\label{ch13:ssec:descriptive-narratives}
The \textbf{"From Graphs to Words"}\index{projects!From Graphs to Words} project is an example of a framework that assists authors in creating rich, narrative descriptions of data visualizations. Instead of being fully automated, these tools work with the author, suggesting key points to highlight and ensuring that the final description is both accurate and insightful \supercite{FromGraphsToWords}.

\section{~~Interactive Graphing Environments and General Accessibility Features}\label{ch13:sec:interactive-environments}
Modern accessible graphing environments\index{software!graphing!interactive} go beyond single-mode solutions and incorporate a range of accessibility features.

\subsection{Keyboard \gidx{navigation}{Navigation} and Structured Exploration}\label{ch13:ssec:keyboard-navigation}
A fundamental requirement for accessibility is full keyboard navigation\index{accessibility!keyboard navigation}. Users must be able to interact with all elements of a chart, including selecting data series, moving between data points, and accessing tooltips, using only the keyboard. Structured exploration\index{data exploration!structured} allows users to navigate a chart logically, for example, by moving from one data point to the next in a series \supercite{AutoVizuA11y}.

\subsection{Multi-Modal Interaction Paradigms}\label{ch13:ssec:multi-modal}
The most effective solutions combine multiple modes of interaction\index{interaction paradigms!multimodal}. A user might explore a tactile graphic with one hand while receiving synchronized sonification and spoken descriptions through headphones. This multi-sensory approach provides a richer and more robust understanding of the data than any single mode alone \supercite{RichScreenReaderExperiences, ReindersAccessibleData}.

\section{~~Infographic Accessibility and Best Practices}\label{ch13:sec:infographics}
Infographics\index{infographics!accessibility} present a unique challenge due to their combination of charts, images, and text in a complex visual layout.

\subsubsection{Describing Complex Visual Information}\label{ch13:sssec:describing-infographics}
Making infographics accessible requires more than a simple alt text\index{alt text!for complex images}. A best practice is to provide a long description that explains the structure of the infographic and walks the reader through the information in a logical sequence. This description should convey not only the individual data points but also the overall narrative and message of the infographic\index{infographics!descriptions} \supercite{CreatingAccessibleInfographics, AdaptingInfographics}.

\subsubsection{Design Principles for Inclusivity}\label{ch13:sssec:inclusive-design}
Inclusive design\index{design!inclusive} principles for infographics\index{infographics!design principles} include:
\begin{itemize}
	\item Using strong color contrast.
	\item Not relying on color alone to convey information.
	\item Using clear, legible fonts.
	\item Ensuring a logical \gidx{readingorder}{reading order} for screen readers.
	\item Providing text alternatives for all non-text elements.
\end{itemize}
\supercite{DoNoHarmGuide, ContrastChecker, WCAG21LevelAA}

\section{~~Cross-Cutting Themes and Future Directions (Legacy Section)}\label{ch13:sec:future-directions-legacy}
Several themes emerge from the review of modern data accessibility tools.

\subsection{The Role of AI and \gidx{machinelearning}{Machine Learning}}\label{ch13:ssec:ai-role}
\gls{AI} and \gls{machinelearning}\index{AI!in data accessibility} are central to the future of data accessibility. They power the \gls{nlg} that creates chart summaries, drive the computer vision that can interpret and describe unstructured images of graphs, and enable the intelligent personalization of multi-modal interfaces to suit individual user needs and preferences \supercite{RECOG, NLGMeaning}.

\subsection{Open-Source Initiatives and Community Collaboration}\label{ch13:ssec:open-source}
Many of the most innovative tools in data accessibility are emerging from open-source projects\index{open source!in accessibility} and academic research. Community collaboration\index{community collaboration} is vital for developing, maintaining, and disseminating these tools, ensuring that they are available to the widest possible audience of educators, developers, and users.

\section{~~Conclusions (Legacy Section)}\label{ch13:sec:conclusions}
The field of data accessibility\index{data accessibility!conclusions} is undergoing a significant transformation, moving away from static, single-mode solutions toward dynamic, multi-modal, and intelligent systems. Technologies like \gidx{tactilegraphics}{tactile graphics}, haptic feedback, sonification, and natural language generation are no longer viewed as separate solutions but as integrated components of a flexible data exploration environment. The research highlighted in this chapter, particularly from institutions like MIT, demonstrates a clear path forward: leveraging AI and user-centered design to create tools that empower visually impaired users to not just consume data, but to interact with it, question it, and derive their own insights. The continued growth of open-source initiatives and a commitment to multi-modal design will be the key drivers in closing the data accessibility gap.

