\chapter{The Evolution and Perceptual Dynamics of Voice Synthesis in Windows Screen Readers}
\section{Executive Summary}
The evolution of text-to-speech (TTS) technology within the Windows
screen reader ecosystem reveals a complex and often paradoxical
landscape. While general advancements in speech synthesis have
prioritized natural, human-like voice quality, the most widely adopted
and enduring voices for screen readers remain those with a distinct,
artificial character. This report provides a detailed analysis of this
trajectory, from the foundational hardware-based systems to the advent
of powerful, cloud-based neural networks. The central finding is the
enduring relevance of low-latency, "robotic" voices like Eloquence,
which are functionally superior for the high-speed data consumption
preferred by expert users. This analysis delves into the trade-offs
between clarity and speed, the nuanced psychological effects of voice
characteristics such as gender, and the current state of the art, which
is characterized by a strategic hybrid model. The future of screen
reader voice synthesis is not a single, universally "natural" voice,
but a versatile ecosystem that empowers users to choose the optimal
audio output for their specific needs, whether for casual listening or
rapid-fire data analysis.

\section{1. The Origins and Foundational Pillars of Screen Reader Voice Synthesis}
The history of speech synthesis is a long and complex journey, tracing
its roots from purely mechanical devices to the digital, software-driven
solutions that are ubiquitous today. This evolution laid the critical
groundwork for the development of modern screen readers, which began to
emerge as essential assistive technologies for people with visual
impairments. The transition from dedicated hardware to integrated
software on the personal computer was a pivotal moment that democratized
access and enabled the rich, customizable experience that users rely on
today.

\subsection{1.1. From the Analog to the Digital (A Brief Preamble)}
Early attempts at replicating human speech were mechanical and required
intricate manual control. In the 18th century, devices like those built
by Christian Gottlieb Kratzenstein could produce the five long vowel
sounds, and Wolfgang von Kempelen's "acoustic-mechanical speech
machine" added consonants to the repertoire.\supercite{1} The first major leap
into the electronic age came in the 1930s with Bell Labs' development
of the vocoder and Homer Dudley's keyboard-operated Voder, which was
famously showcased at the 1939 New York World's Fair.\supercite{1} This period
demonstrated that electronic speech synthesis was possible, but the
devices were unwieldy and required extensive human training to operate.

The first computer-based speech synthesis systems emerged in the late
1950s.\supercite{1} A landmark moment occurred in 1961 when Bell Labs physicists
John Larry Kelly, Jr and Louis Gerstman used an IBM 704 to synthesize
the song "Daisy Bell," an event that later inspired the chilling death
scene of HAL 9000 in

*2001: A Space Odyssey*.\supercite{2} Over time, these systems became more
efficient, leading to the development of Linear Predictive Coding (LPC)
by researchers at NTT and Bell Labs in the 1970s.\supercite{1} LPC became the
basis for early speech synthesizer chips, such as those used in the
iconic Texas Instruments Speak & Spell toy in 1978.\supercite{1} These devices
marked the first time the technology became accessible to a mass
consumer market, albeit in a highly specialized application.

During this period, early all-software voice programs were also
developed for home computers. The Software Automatic Mouth (SAM),
released in 1982, was a significant example, running on non-Windows
platforms like the Apple II, various Atari models, and the Commodore
64.\supercite{1} These early systems often required specific hardware, like DACs
or embedded sound chips, and the quality was highly dependent on the
computer's architecture.\supercite{1} The landscape was dominated by
hardware-centric or proprietary systems. For instance, the DECtalk,
commercialized in 1983, was a \$4,000 standalone unit, which highlighted
that speech output was initially an expensive, specialized add-on rather
than a core component of the personal computing experience.\supercite{2} This
proprietary and costly environment set the stage for the next phase of
development.

\subsection{1.2. The Windows Catalyst: Screen Readers Find Their Voice}
The screen reader as an assistive technology for blind and visually
impaired individuals has played a pivotal role in enabling them to
interact with digital environments independently.\supercite{3} The earliest
commercial screen readers, such as Versatile Speech from the early
1980s, were hardware-based solutions that required a physical connection
to a computer to function.\supercite{3} This approach had significant limitations,
which led to a notable transition in the late 1980s and early 1990s
toward software-based solutions designed for text-based interfaces like
MS-DOS.\supercite{3} This shift offered greater flexibility and integration with
existing computer systems.

A major turning point in the evolution of screen readers occurred with
the debut of Microsoft Windows 3.0 in 1990.\supercite{3} The graphical user
interface (GUI) of Windows presented a significant challenge for screen
readers, which were designed to handle text-only environments. To
address this, software developers created innovative solutions like JAWS
(Job Access With Speech).\supercite{3} JAWS pioneered the "off-screen model,"
which created a virtual text representation of the graphical
environment, allowing users to navigate the GUI with text-to-speech or
Braille displays.\supercite{3} This was a critical technological leap that
maintained accessibility despite the increasing complexity of operating
systems.

The release of JAWS for Windows in 1995 was a landmark event, but a
truly transformative milestone occurred in 1998 with the introduction of
its "First Built-in Software Synthesizer".\supercite{4} This was a game-changer
for the assistive technology market. By integrating a synthesizer that
could run on the computer's sound card, JAWS eliminated the need for a
user to purchase a separate, expensive hardware unit.\supercite{4} This single
action fundamentally reduced the cost and complexity of screen reader
adoption. It transitioned the technology from a highly specialized,
hardware-dependent niche to a more accessible software application,
marking a key moment in the history of accessibility as the technology
became more widely available and democratized.

\subsection{1.3. The Defining Voices: Eloquence, eSpeak, and Vocalizer}
The choice of a default voice engine for a screen reader is not merely a
technical decision; it is a statement of product philosophy. The leading
screen readers for Windows---JAWS, NVDA, and SuperNova---each adopted
engines that reflect different priorities, and these choices have
profoundly shaped user preferences and expectations.

The ETI-Eloquence synthesizer, with its first version released in Italy
in 1993, became a defining voice for many JAWS users.\supercite{5} This
software-based, multi-voice system generates waveforms algorithmically
rather than by concatenating sound files, which is a key factor in its
low computational footprint and high efficiency.\supercite{6} It was built on
"diphone-technology aimed to reach a high computational
efficiency".\supercite{5} This focus on a rule-based, efficient approach directly
translates to its highly responsive and low-latency performance.

NVDA (NonVisual Desktop Access), a widely used free and open-source
screen reader, defaults to its own built-in eSpeak voice.\supercite{8} While some
users describe eSpeak's sound as "a little tinny" \supercite{10}, it shares a
core design philosophy with Eloquence by prioritizing functional
efficiency, particularly for high-speed use.\supercite{11} This minimalist,
lightweight approach makes it an ideal core voice for a free, portable
screen reader.\supercite{9} NVDA's open-source nature, however, also provides a
flexible architecture that allows it to seamlessly integrate other
voices, such as the default Microsoft Speech API 5 voices that are
bundled with Windows.\supercite{10}

In contrast, the SuperNova screen reader explicitly features the "high
quality, human sounding, Vocalizer Expressive text-to-speech
synthesiser" as a core component of its Magnifier & Speech edition.\supercite{12}
This choice reflects a product strategy that prioritizes a comfortable,
natural-sounding experience, which may be particularly appealing to a
user base that includes people with low vision who rely on a combination
of magnification and speech output.\supercite{12} The disparate choices of default
voice engines for JAWS (Eloquence), NVDA (eSpeak), and SuperNova
(Vocalizer) represent a fundamental philosophical divide between
functional speed and auditory comfort. This tension is central to the
user experience and continues to define the evolution of screen reader
technology.

Table 1: Key Milestones in Windows Screen Reader Voice History

  -----------------------------------------------------------------------
  Year                    Event                   Significance
  ----------------------- ----------------------- -----------------------
  Early 1980s             Versatile Speech        Inception of commercial
                          (hardware-based screen  screen readers, but
                          reader) emerges         limited by physical
                                                  hardware dependencies.

  Late 1980s-Early 1990s  Transition to           Shift to software
                          software-based          programs offered
                          solutions for MS-DOS    greater flexibility and
                                                  integration.

  1990                    Debut of Microsoft      GUI poses a new
                          Windows 3.0             challenge for
                                                  text-based screen
                                                  readers, prompting
                                                  innovative solutions.

  1993                    First version of        Introduction of a
                          Eloquence released by   high-efficiency,
                          CSELT                   diphone-based software
                                                  synthesizer.

  1995                    JAWS for Windows        First major screen
                          version 1 released      reader for the Windows
                                                  graphical environment.

  1998                    JAWS includes its       Eliminates the need for
                          "First Built-in        a separate hardware
                          Software Synthesizer"  synthesizer,
                                                  democratizing access.

  2000                    Microsoft Sam is        Integration of a basic,
                          default TTS for Windows free screen reader into
                          Narrator                the operating system.

  2006                    NVDA (NonVisual Desktop Introduction of a free,
                          Access) released        open-source alternative
                                                  with a built-in eSpeak
                                                  voice.

  Mid-2010s               SuperNova integrates    Adoption of a more
                          Vocalizer Expressive    modern,
                                                  natural-sounding voice
                                                  as a core offering.

  Present                 Integration of          Screen readers begin to
                          cloud-based neural      leverage AI for more
                          voices (e.g., Azure,    expressive,
                          Polly, Google)          high-quality, but
                                                  higher-latency output.
  -----------------------------------------------------------------------

\section{2. A Comparative Analysis of Synthesizer Performance: The Clarity-Speed Dichotomy}
The performance of a text-to-speech engine for a screen reader cannot be
judged solely by its ability to sound like a human. For a significant
portion of the user base, particularly advanced users, the most critical
metric is high-speed intelligibility and responsiveness. This creates a
fascinating dichotomy where legacy, "robotic" voices often outperform
their modern, human-sounding counterparts.

\subsection{2.1. The Critical Metric: High-Speed Intelligibility}
The average sighted person reads at a speed of 200 to 300 words per
minute (WPM), while the average human speaking rate is around 150
WPM.\supercite{14} As a result, a user who is visually oriented might find it
tedious to listen to a text at the same speed they could read it
visually.\supercite{14} This phenomenon is even more pronounced for experienced
screen reader users. Instead of a natural speaking rate, expert users
consistently set their screen readers to speeds of 350 WPM or faster,
with some power users operating at 500 WPM or more.\supercite{14} One user
reported listening to their screen reader at speeds as high as 800
WPM.\supercite{16} This level of speed is so high that a sighted listener who is
not accustomed to the technology finds it "incomprehensible".\supercite{17}

The reason for this speed preference is a fundamental difference in the
way information is consumed. For a power user, the screen reader is not
a substitute for an audiobook; it is a tool for rapid data ingestion and
navigation.\supercite{15} The user is not passively listening to a story; they are
actively "scanning" for headings, links, and keywords, much like a
sighted user would visually skim a webpage.\supercite{15} The speed at which this
"auditory scanning" can be performed is directly tied to the user's
productivity and efficiency. The purpose of the auditory information has
been redefined from a medium for passive comprehension to a high-speed
interface for data analysis. This profound cognitive difference explains
why a technology that prioritizes comfort and naturalness is often
ill-suited for the most demanding use cases.

\subsection{2.2. The Eloquence Benchmark: Low-Latency for High-Speed}
The enduring popularity of the Eloquence voice among power users is not
a matter of sentimentality; it is a direct consequence of its functional
superiority for high-speed use. The primary advantage of Eloquence is
its near-instant responsiveness, with a latency of less than 50
milliseconds from the moment text is sent to the synthesizer until the
audio is output.\supercite{11} This is a direct result of its algorithmic,
rule-based design, which generates speech from a small, lightweight
footprint of just over 10 MB for multiple languages.\supercite{7}

In stark contrast, more modern, natural-sounding voices---including
those based on concatenative or neural network models---can have a
latency ranging from 100 to 500 milliseconds, or even up to a full
second in some cases.\supercite{11} This performance bottleneck is a significant
impediment for a user navigating a complex interface. A user who needs
to press a key and receive immediate auditory feedback on their location
or a new element finds a half-second delay intolerable. While newer
voices may sound subjectively better at natural speaking rates, they are
reported to "fail epically" at high-speed reading.\supercite{11} This represents
a paradox of technological advancement: the quest for "naturalness"
introduces performance penalties that degrade the core function of the
technology for its most demanding users. The optimal solution is not
simply "more human-like" but "more functionally-aligned" with the
user's need for precision and speed.

\subsection{2.3. Performance Across the Spectrum: A Comparative View}
While legacy voices like Eloquence remain the gold standard for pure
speed and responsiveness, modern synthesizers have made significant
strides in their own right. A study evaluating synthesized voices found
that some from Amazon and Google were even more intelligible than human
voices when exposed to speech-shaped noise.\supercite{18} The study also provided
quantitative data on performance, with voices like Amazon Stephen and
Google C achieving human-like recognition in quiet environments and
producing speech at rates of 208 and 190 WPM respectively.\supercite{18} These
results indicate that the gap between legacy and modern voices is
narrowing, at least at the lower end of the "high-speed" spectrum
(e.g., up to 200 WPM).

However, user anecdotes of operating at 500 WPM or higher with Eloquence
suggest that a significant performance difference still exists at the
extreme end of the speed scale.\supercite{15} The modern voices, while capable of
high-speed delivery, still cannot match the sheer velocity and minimal
latency of a purpose-built, legacy voice. The availability of
high-quality voices like Vocalizer Expressive within screen readers like
SuperNova, and the ability of JAWS and NVDA to use them, means that
users are not locked into a single experience.\supercite{10} This allows for a
hybrid approach where a user can select a more comfortable, natural
voice for tasks like reading a long document at a moderate pace, but
instantly switch to a high-speed, low-latency voice for navigating a
complex application or website.

Table 2: Screen Reader Voice Engine Performance by Speaking Rate and
User Preference

  -----------------------------------------------------------------------------------------------------
  Voice Engine    Core Technology        Perceived     Perceived   Latency       Target     Footprint
                                         Naturalness   Clarity                   User       
                                         (Low Speed)   (High                                
                                                       Speed)                               
  --------------- ---------------------- ------------- ----------- ------------- ---------- -----------
  Eloquence   Algorithmic,           Robotic,      Excellent   Very Low      Expert     Small (10
                  Rule-based             artificial    (500+ WPM)  (\<50ms)                 MB)

  eSpeak      Algorithmic,           Tinny,        Good        Low           Expert     Small
                  Rule-based             artificial                                         

  Vocalizer     Concatenative/Hybrid   Natural,      Good (up to Medium        Both       Medium
  Expressive                           human-like    300 WPM)    (100-200ms)              

  Microsoft     Concatenative          Slightly      Fair        Medium        Novice     Small
  SAPI                                 robotic                                            

  Microsoft     Neural Network         Very natural, Fair (200+  High          Novice     Large
  Azure Neural                         expressive    WPM)        (\>200ms)                (Cloud)

  Amazon Polly  Neural Network         Very natural, Fair (200+  High          Novice     Large
  Neural                               expressive    WPM)        (\>200ms)                (Cloud)
  -----------------------------------------------------------------------------------------------------

\section{3. The Psychological and Perceptual Dimensions of Voice Synthesis}
Beyond technical specifications like speed and latency, the user
experience of screen reader voices is profoundly influenced by
psychological and perceptual factors. These elements determine not only
which voices a user prefers but also how they interact with the
technology on a subconscious level.

\subsection{3.1. The User Journey: From Naturalness to Efficiency}
The journey of a screen reader user, from novice to expert, is often
marked by a significant shift in voice preference. A study examining
screen reader adoption found that a primary driver for early users is a
"human-sounding voice".\supercite{19} This initial preference suggests that new
users are seeking a familiar and comfortable auditory experience. A
natural-sounding voice reduces the cognitive load associated with
learning a new technology and its complex navigation schemes. For a
novice, the primary challenge is decoding the auditory information, and
a voice that sounds like a human speaker makes this task easier and less
fatiguing.

However, as users gain experience, their needs and cognitive demands
change. As they become more comfortable with screen reader navigation,
they begin to increase their speaking rate, and as a result, they become
accustomed to and even prefer the "non-human-sounding speech" that is
a hallmark of high-speed output.\supercite{19} The primary bottleneck is no longer
understanding the voice but the speed at which information can be
delivered. At this stage, the user is no longer merely listening to a
voice; they are "reading with their ears." Any latency, prosodic
fluctuation, or distracting human-like nuance becomes an impediment to
efficiency. This transition in preference reflects a change in the
user's cognitive model from one of passive listening to one of active,
high-speed data consumption.

\subsection{3.2. Voice Gender and Understandability: A Confounding Factor}
The choice of a voice's gender is a complex issue in speech synthesis,
with its effects on perceived clarity and user behavior being a subject
of ongoing research. Studies on this topic present mixed and contextual
results. One study found no significant difference in intelligibility
between the male and female voices of the DECtalk system, though the
male voice was more negatively affected by noise.\supercite{20} Another study's
findings suggested that the effect of gender was dependent on the
specific TTS system and the presence of noise, indicating a complex
interaction of factors.\supercite{21} This evidence indicates that simple,
universal conclusions about the intrinsic intelligibility of male or
female synthetic voices are not possible.

Furthermore, the impact of voice gender extends beyond simple technical
clarity. It is deeply intertwined with social biases. A study on
AI-powered voice assistants like Siri and Alexa found that male users
interrupted feminine-voiced assistants almost twice as often as they did
masculine or gender-neutral ones.\supercite{22} The researchers concluded that
this behavior may reflect and reinforce real-life gendered power
dynamics, and that using a gender-neutral voice could promote more
respectful human-to-human interactions.\supercite{22} The fact that a single voice
characteristic can trigger a subconscious, biased behavior suggests that
voice selection for screen readers should not be treated as a neutral
variable.

This is further complicated by generational changes in perception. A
study on adolescents found no statistical preference for either male or
female narrators in auditory advertisements, suggesting that younger
generations hold more egalitarian views on gender roles.\supercite{23} Given these
factors, developers must navigate a difficult choice: selecting a voice
for a screen reader involves considering not only technical performance
but also the potential for unconscious social bias and the evolving
perceptions of a diverse user base.

\section{4. The Current State of the Art and Future Trajectory: The Rise of AI}
The field of text-to-speech has been revolutionized by the application
of artificial intelligence and deep learning, which are now enabling a
new generation of voice synthesis that is more natural, expressive, and
intelligent than ever before. This new frontier is changing the role of
the screen reader, transforming it from a simple text-to-audio converter
into a sophisticated, AI-powered interface.

\subsection{4.1. The New Frontier: Cloud-Based Neural Synthesis}
Leading technology companies now offer powerful, cloud-based TTS
services. Amazon Polly \supercite{24^, Microsoft Azure AI Speech ^25}, and Google
Cloud TTS \supercite{26} utilize deep learning and neural networks to generate
voices that are described as "lifelike," with natural breathing
patterns, emotional nuance, and contextual emphasis.\supercite{24} Google's
latest voices, for example, incorporate "human disfluencies" and
accurate intonation to produce a highly conversational and realistic
sound.\supercite{26}

This shift to neural synthesis transforms the screen reader's
capabilities. It can now go beyond merely reading text. For instance,
Microsoft Narrator is already leveraging AI to provide rich, detailed
descriptions of images, charts, and graphs.\supercite{28} This enables the screen
reader to interpret and articulate visual information that was
previously inaccessible, representing a profound leap in functionality.
This transformation, however, introduces new challenges for the user.
These AI-powered services often require an active internet
connection.\supercite{28} Furthermore, their pricing models are typically based on
the number of characters converted to speech \supercite{25}, which could pose a
financial barrier for high-volume users who read hundreds of thousands
of words per day.

\subsection{4.2. Bridging the Gap: Integration and Adoption}
Despite the power of cloud-based neural voices, screen reader developers
are not simply replacing legacy voices. They are creating a
sophisticated, hybrid ecosystem that leverages the best of both worlds.
They are actively integrating with powerful, high-latency cloud services
while still maintaining the low-latency, high-speed functionality of
legacy voices for power users.

This "dual path" approach is evident in the ongoing development of
screen readers. JAWS is actively training users on how to use AI for
tasks like summarizing data and generating content.\supercite{29} It is also
adding new features like automatic language detection, which adjusts the
voice based on the written language in real-time.\supercite{30} The open-source
nature of NVDA, supported by a community of developers and add-ons,
makes it a highly flexible platform for integrating a wide range of
third-party voices, including those from Microsoft's Speech API and
other providers.\supercite{10} The technical details of these integrations often
involve API keys and endpoint URIs, requiring a client-server connection
to access the cloud-based services.\supercite{32}

The coexistence of these technologies ensures that the screen reader can
serve both the user who wants a comfortable, natural voice for a casual
task and the expert who needs a fast, functional voice for a complex
task. This hybrid model represents a pragmatic and user-centric approach
that ensures the technology can evolve while remaining true to the
foundational needs of its most dedicated user base.

Table 3: Cloud-Based Neural Voice Engines for Screen Readers

  ---------------------------------------------------------------------------------
  Service        Core Features  Pricing Model    Screen Reader  Noteworthy
                                                 Integration    Technology
  -------------- -------------- ---------------- -------------- -------------------
  Microsoft    Emotional      Pay-as-you-go,   Native         OpenAI Whisper
  Azure AI       nuance, custom based on         integration    support. Can run
  Speech       neural voices, characters. Free via            on-device with
                 100+           tier available.  SAPI/Windows   embedded models.
                 languages,                      Narrator.      
                 SSML support.                   Developers can 
                                                 use SDKs.      

  Amazon       Lifelike       Pay-as-you-go,   Third-party    Deploys a
  Polly        voices, custom based on         developers can billion-parameter
                 lexicons, 100+ characters. Free integrate via  transformer to
                 voices, 40+    tier available.  API. No native generate voices.
                 languages,                      NVDA/JAWS      Caching available
                 SSML support.                   support.       for faster
                                                                retrieval.

  Google Cloud Humanlike      Pay-as-you-go,   Third-party    Features "Chirp
  TTS          intonation,    based on         developers can 3" voices with
                 380+ voices,   characters. Free integrate via  human disfluencies.
                 50+ languages, tier available.  API. No native Instant custom
                 SSML support.                   NVDA/JAWS      voice creation from
                                                 support.       10 seconds of
                                                                audio.
  ---------------------------------------------------------------------------------

\section{5. Recommendations and Conclusion}
\subsection{5.1. Recommendations for Developers}
The analysis of the screen reader TTS landscape leads to several key
recommendations for developers. First, it is essential to continue to
maintain and optimize legacy, low-latency voices like Eloquence. For
power users, the functional efficiency of these voices is a
non-negotiable feature. Second, when integrating cloud-based neural
voices, developers should prioritize the creation of robust, low-latency
APIs and smart caching systems to mitigate the impact of network
latency.\supercite{33} Third, the user interface should provide clear and
accessible options for users to switch between voice types, rates, and
verbosity settings to match the task at hand.\supercite{15} Finally, developers
should proactively consider the social implications of their voice
choices and offer gender-neutral options to create a more equitable user
experience.

\subsection{5.2. Recommendations for End Users}
For end users, the most valuable recommendation is to approach voice
selection with a clear understanding of the trade-offs involved. Users
should experiment with both legacy voices and modern, natural-sounding
ones to discover what works best for different contexts, such as a
high-speed work session versus a relaxed reading of a long document.
Users can also leverage the flexibility of screen readers like NVDA by
exploring community-developed add-ons, which can provide access to new
voices and expanded functionality.\supercite{31} The use of advanced features like
speech history and pronunciation dictionaries can also enhance their
experience and allow for a more personalized interaction with the
technology.\supercite{15}

\subsection{5.3. Conclusion}
The evolution of screen reader voice synthesis is a powerful testament
to the influence of user-driven innovation. The paradox of the robotic
voice, where a seemingly outdated technology remains functionally
superior for a specific, demanding use case, demonstrates that the
concept of "good" speech is not universal. It is defined by the
user's needs and context. The transition of the user from a novice
seeking comfort and naturalness to an expert prioritizing speed and
efficiency is a central phenomenon that has shaped this entire field.
The future of this technology lies in a sophisticated, hybrid ecosystem
that moves beyond a single, one-size-fits-all solution. The ultimate
goal is to provide a versatile and intelligent system that offers a
spectrum of choices, empowering each user to navigate the digital world
with a voice that is perfectly tuned to their unique perception and
operational demands.

\subsection{Works cited}
1.  Speech synthesis - Wikipedia, accessed August 26, 2025,
    [[https://en.wikipedia.org/wiki/Speech_synthesis]{.underline}](https://en.wikipedia.org/wiki/Speech_synthesis)

2.  The Complete Evolution of Text to Speech Technology - LyricWinter,
    accessed August 26, 2025,
    [[https://lyricwinter.com/blog/complete-evolution-text-to-speech-technology]{.underline}](https://lyricwinter.com/blog/complete-evolution-text-to-speech-technology)

3.  The Evolution of Screen Readers for Blind Users - outreach1,
    accessed August 26, 2025,
    [[https://www.outreach1.org/2025/07/13/the-evolution-of-screen-readers-for-blind-users/]{.underline}](https://www.outreach1.org/2025/07/13/the-evolution-of-screen-readers-for-blind-users/)

4.  JAWS Timeline - Vispero, accessed August 26, 2025,
    [[https://vispero.com/jaws-timeline/]{.underline}](https://vispero.com/jaws-timeline/)

5.  Eloquens (software) - Wikipedia, accessed August 26, 2025,
    [[https://en.wikipedia.org/wiki/Eloquens\_(software)]{.underline}](https://en.wikipedia.org/wiki/Eloquens_(software))

6.  ETI-Eloquence, accessed August 26, 2025,
    [[http://svr-www.eng.cam.ac.uk/comp.speech/Section5/Synth/eloquence.html]{.underline}](http://svr-www.eng.cam.ac.uk/comp.speech/Section5/Synth/eloquence.html)

7.  LANGUAGE-UNIVERSAL AND LANGUAGE-SPECIFIC COMPONENTS IN THE
    MULTI-LANGUAGE ETI-ELOQUENCE TEXT-TO-SPEECH SYSTEM, accessed August
    26, 2025,
    [[https://www.internationalphoneticassociation.org/icphs-proceedings/ICPhS1999/papers/p14_2283.pdf]{.underline}](https://www.internationalphoneticassociation.org/icphs-proceedings/ICPhS1999/papers/p14_2283.pdf)

8.  JAWS vs NVDA: Which Is Better for Accessibility Audits? - Equally.AI
    Blog, accessed August 26, 2025,
    [[https://blog.equally.ai/disability-guide/jaws-vs-nvda/]{.underline}](https://blog.equally.ai/disability-guide/jaws-vs-nvda/)

9.  About NVDA - NV Access, accessed August 26, 2025,
    [[https://www.nvaccess.org/about-nvda/]{.underline}](https://www.nvaccess.org/about-nvda/)

10. NVDA Screen Reader - A Tutorial for sighted testers -
    Netz-barrierefrei.de, accessed August 26, 2025,
    [[https://netz-barrierefrei.de/en/nvda-testing.html]{.underline}](https://netz-barrierefrei.de/en/nvda-testing.html)

11. Why Eloquence? - chat@nvda.groups.io, accessed August 26, 2025,
    [[https://nvda.groups.io/g/chat/topic/why_eloquence/105595533]{.underline}](https://nvda.groups.io/g/chat/topic/why_eloquence/105595533)

12. SuperNova Magnifier & Speech - LVI America, accessed August 26,
    2025,
    [[https://lviamerica.com/catalog/products/supernova-reader-magnifier]{.underline}](https://lviamerica.com/catalog/products/supernova-reader-magnifier)

13. SuperNova - Dolphin Computer Access, accessed August 26, 2025,
    [[https://yourdolphin.com/SuperNova]{.underline}](https://yourdolphin.com/SuperNova)

14. screen reader and Speech output - why is it so fast and sounds
    artificial?, accessed August 26, 2025,
    [[https://www.netz-barrierefrei.de/en/screenreader-output.html]{.underline}](https://www.netz-barrierefrei.de/en/screenreader-output.html)

15. Information Wayfinding of Screen Reader Users: Five Personas to
    Expand Conceptualizations of User Experiences - PMC, accessed August
    26, 2025,
    [[https://pmc.ncbi.nlm.nih.gov/articles/PMC11872227/]{.underline}](https://pmc.ncbi.nlm.nih.gov/articles/PMC11872227/)

16. Is there a difference for how fast you can listen to the newer more
    nicely sou\... \| Hacker News, accessed August 26, 2025,
    [[https://news.ycombinator.com/item?id=44719112]{.underline}](https://news.ycombinator.com/item?id=44719112)

17. The Voices of Screen Readers - Tamman Inc, accessed August 26, 2025,
    [[https://tammaninc.com/learn/the-voices-of-screen-readers/]{.underline}](https://tammaninc.com/learn/the-voices-of-screen-readers/)

18. Evaluating synthesized speech intelligibility in noise \| JASA
    Express Letters - AIP Publishing, accessed August 26, 2025,
    [[https://pubs.aip.org/asa/jel/article/5/4/045202/3344397/Evaluating-synthesized-speech-intelligibility-in]{.underline}](https://pubs.aip.org/asa/jel/article/5/4/045202/3344397/Evaluating-synthesized-speech-intelligibility-in)

19. The "voice" has it: screen reader adoption and switching behavior
    among vision impaired persons in India, accessed August 26, 2025,
    [[https://pmc.ncbi.nlm.nih.gov/articles/PMC3955012/]{.underline}](https://pmc.ncbi.nlm.nih.gov/articles/PMC3955012/)

20. Segmental intelligibility and speech interference thresholds of
    high-quality synthetic speech in presence of noise - PubMed,
    accessed August 26, 2025,
    [[https://pubmed.ncbi.nlm.nih.gov/8377491/]{.underline}](https://pubmed.ncbi.nlm.nih.gov/8377491/)

21. implicit measurement of intelligibility of male and female voice
    text-to-speech (tts) synthesis in noise using a phoneme detection
    task, accessed August 26, 2025,
    [[https://assta.org/proceedings/sst/sst2002/Papers/Lees059.pdf]{.underline}](https://assta.org/proceedings/sst/sst2002/Papers/Lees059.pdf)

22. Alexa, should voice assistants have a gender? - JHU Hub - Johns
    Hopkins University, accessed August 26, 2025,
    [[https://hub.jhu.edu/2025/01/16/alexa-should-voice-assistants-have-a-gender/]{.underline}](https://hub.jhu.edu/2025/01/16/alexa-should-voice-assistants-have-a-gender/)

23. (PDF) Adolescents Voice Preference in Auditory Advertisements: A
    Study in Gender Stereotypes and Multi-Media Marketing -
    ResearchGate, accessed August 26, 2025,
    [[https://www.researchgate.net/publication/350551580_Adolescents_Voice_Preference_in_Auditory_Advertisements_A_Study_in_Gender_Stereotypes_and_Multi-Media_Marketing]{.underline}](https://www.researchgate.net/publication/350551580_Adolescents_Voice_Preference_in_Auditory_Advertisements_A_Study_in_Gender_Stereotypes_and_Multi-Media_Marketing)

24. AI Voice Generator and Text-to-Speech Tool - Amazon Polly, accessed
    August 26, 2025,
    [[https://aws.amazon.com/polly/]{.underline}](https://aws.amazon.com/polly/)

25. Azure AI Speech \| Microsoft Azure, accessed August 26, 2025,
    [[https://azure.microsoft.com/en-us/products/ai-services/ai-speech]{.underline}](https://azure.microsoft.com/en-us/products/ai-services/ai-speech)

26. Text-to-Speech AI: Lifelike Speech Synthesis - Google Cloud,
    accessed August 26, 2025,
    [[https://cloud.google.com/text-to-speech]{.underline}](https://cloud.google.com/text-to-speech)

27. Neural Speech Synthesis 2.0: Natural Voice Technology Transcends
    Current Audio Accessibility, accessed August 26, 2025,
    [[https://accessible.org/neural-speech-synthesis-2-0-natural-voice-technology-audio-accessibility/]{.underline}](https://accessible.org/neural-speech-synthesis-2-0-natural-voice-technology-audio-accessibility/)

28. Complete guide to Narrator - Microsoft Support, accessed August 26,
    2025,
    [[https://support.microsoft.com/en-us/windows/complete-guide-to-narrator-e4397a0d-ef4f-b386-d8ae-c172f109bdb1]{.underline}](https://support.microsoft.com/en-us/windows/complete-guide-to-narrator-e4397a0d-ef4f-b386-d8ae-c172f109bdb1)

29. Freedom Scientific Blog -- News from the cutting edge of assistive
    technology, accessed August 26, 2025,
    [[https://blog.freedomscientific.com/]{.underline}](https://blog.freedomscientific.com/)

30. What's New in JAWS 2025 Screen Reading Software - Freedom
    Scientific, accessed August 26, 2025,
    [[https://support.freedomscientific.com/downloads/jaws/JAWSWhatsNew]{.underline}](https://support.freedomscientific.com/downloads/jaws/JAWSWhatsNew)

31. Native Speech History in NVDA · Issue #18400 · nvaccess/nvda -
    GitHub, accessed August 26, 2025,
    [[https://github.com/nvaccess/nvda/issues/18400]{.underline}](https://github.com/nvaccess/nvda/issues/18400)

32. Activate and configure the Microsoft Azure TTS integration, accessed
    August 26, 2025,
    [[https://help.mypurecloud.com/articles/activate-and-configure-the-microsoft-azure-tts-integration/]{.underline}](https://help.mypurecloud.com/articles/activate-and-configure-the-microsoft-azure-tts-integration/)

33. UMS Polly - Release Notes - UniMRCP Documentation, accessed August
    26, 2025,
    [[https://docs.unispeech.io/en/ums/aws/polly/releases]{.underline}](https://docs.unispeech.io/en/ums/aws/polly/releases)

34. Learn NVDA: Hotkeys and Commands, Part 2 - The American Foundation
    for the Blind, accessed August 26, 2025,
    [[https://www.afb.org/blindness-and-low-vision/using-technology/assistive-technology-videos/learn-nvda/hotkeys-and-0]{.underline}](https://www.afb.org/blindness-and-low-vision/using-technology/assistive-technology-videos/learn-nvda/hotkeys-and-0)

35. 5.5.1 Speech History - Freedom Scientific, accessed August 26, 2025,
    [[https://support.freedomscientific.com/teachers/lessons/5.5.1_SpeechHistory.htm]{.underline}](https://support.freedomscientific.com/teachers/lessons/5.5.1_SpeechHistory.htm)
